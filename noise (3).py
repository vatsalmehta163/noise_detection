# -*- coding: utf-8 -*-
"""Noise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w4kB4JAF_nmc6mUdW0wViqJ2KBPehuLW

Import necessary packages
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

filename='/content/drive/MyDrive/archive/clean_trainset_wav/p226_007.wav'

import IPython.display as ipd
import librosa
import librosa.display

!dir

"""Visualize"""

plt.figure(figsize=(14, 5))
data, sample_rate = librosa.load(filename)
librosa.display.waveshow(data, sr=sample_rate)
ipd.Audio(filename)

filename='/content/drive/MyDrive/archive/clean_trainset_wav/p226_007.wav'
import matplotlib.pyplot as plt # Import the pyplot module
plt.figure(figsize=(14,5))
data,sample_rate=librosa.load(filename)
librosa.display.waveshow(data,sr=sample_rate)
import IPython.display as ipd
ipd.Audio(filename)

sample_rate

from scipy.io import wavfile as wav
wave_sample_rate, wave_audio=wav.read(filename)

wave_sample_rate

### Let's read a sample audio using librosa
import librosa
audio_file_path='/content/drive/MyDrive/archive/clean_trainset_wav/p226_011.wav'
librosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)

print(librosa_audio_data)

### Lets plot the librosa audio data
import matplotlib.pyplot as plt
# Original audio with 1 channel
plt.figure(figsize=(12, 4))
plt.plot(librosa_audio_data)

### Lets read with scipy
from scipy.io import wavfile as wav
wave_sample_rate, wave_audio = wav.read(audio_file_path)

wave_audio

import matplotlib.pyplot as plt

# Original audio with 2 channels
plt.figure(figsize=(12, 4))
plt.plot(wave_audio)

"""Data Exploration"""

import os
import librosa
import numpy as np
import soundfile as sf
import matplotlib.pyplot as plt

# Path to dataset folder
DATASET_PATH = "/content/drive/MyDrive/archive"

# List all audio files in the dataset folder
audio_files = [os.path.join(DATASET_PATH, f) for f in os.listdir(DATASET_PATH) if f.endswith('.wav')]
#labels = [1 if "clean" in f else 0 for f in audio_files]
labels = [1 if "clean" in os.path.dirname(f) else 0 for f in audio_files]  # Adjust to your dataset's structure
# Display some basic information about the dataset
print(f"Total audio files: {len(audio_files)}")

# Check if audio_files is empty before accessing elements
if audio_files:
    print(f"Sample audio file: {audio_files[0]}")

    # Function to load and display basic properties of an audio file
    def explore_audio(file_path):
        audio, sr = librosa.load(file_path, sr=None)  # Load with the original sampling rate
        duration = librosa.get_duration(y=audio, sr=sr)
        print(f"File: {file_path}")
        print(f"Sampling Rate: {sr} Hz")
        print(f"Duration: {duration:.2f} seconds")
        return audio, sr

    # Test with one file
    audio, sr = explore_audio(audio_files[0])
else:
    print("No audio files found in the specified directory.")

"""Train-Test-Split"""

from sklearn.model_selection import train_test_split
import os

# Path to dataset folder (Make sure it's correct)
DATASET_PATH = "/content/drive/MyDrive/archive"

# List all audio files in the dataset folder
audio_files = [os.path.join(DATASET_PATH, f) for f in os.listdir(DATASET_PATH) if f.endswith('.wav')]
labels = [1 if "clean" in f else 0 for f in audio_files]

# Check if audio_files is not empty
if audio_files:
    # Ensure proper shuffling to avoid biases
    combined = list(zip(audio_files, labels))
    np.random.seed(42)  # For reproducibility
    np.random.shuffle(combined)

    audio_files, labels = zip(*combined)

    # Split into training and testing sets
    train_files, test_files, labels_train, labels_test = train_test_split(
        audio_files, labels, test_size=0.2, stratify=labels, random_state=42
    )
else:
    print("No .wav files found in the specified directory. Please check your DATASET_PATH.")

from sklearn.model_selection import train_test_split
import os

# Path to the root dataset folder
DATASET_PATH = "/content/drive/MyDrive/archive"

# Function to recursively list all .wav files with their labels
def get_audio_files_with_labels(dataset_path):
    audio_files = []
    labels = []
    for root, _, files in os.walk(dataset_path):  # Recursively walk through directories
        for file in files:
            if file.endswith(".wav"):  # Check for .wav files
                file_path = os.path.join(root, file)
                audio_files.append(file_path)
                # Assign labels based on folder name
                if "clean" in root.lower():
                    labels.append(1)  # Label 1 for "clean" audio
                elif "noisy" in root.lower():
                    labels.append(0)  # Label 0 for "noisy" audio
    return audio_files, labels

# Get all audio files and their corresponding labels
audio_files, labels = get_audio_files_with_labels(DATASET_PATH)

# Check if any files were found
if audio_files:
    print(f"Found {len(audio_files)} audio files.")

    # Ensure proper shuffling to avoid biases
    combined = list(zip(audio_files, labels))
    np.random.seed(42)  # For reproducibility
    np.random.shuffle(combined)
    audio_files, labels = zip(*combined)

    # Split into training and testing sets
    train_files, test_files, train_labels, test_labels = train_test_split(
        audio_files, labels, test_size=0.2, stratify=labels, random_state=42
    )

    print(f"Training files: {len(train_files)}, Testing files: {len(test_files)}")
else:
    print("No .wav files found in the specified directory. Please check your DATASET_PATH.")

"""Re-sampling and Pre-Processing"""

def preprocess_audio(file_path, target_sr=16000, target_length=3):

    try:
        audio, sr = librosa.load(file_path, sr=None)

        # Resample to target sampling rate
        if sr != target_sr:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)# Ensures consistency across all audio data

        # Pad/truncate to target length ,Ensures uniform input size
        num_samples = int(target_sr * target_length)
        if len(audio) < num_samples:
            audio = np.pad(audio, (0, num_samples - len(audio)), 'constant')
        else:
            audio = audio[:num_samples]

        # Normalize audio ,Normalizes the audio to have values between -1 and 1.
        audio = audio / np.max(np.abs(audio))
        return audio

    except Exception as e:
        print(f"Error processing file {file_path}: {e}")
        return None

def segment_audio(audio, sr, segment_length=3):
    segment_samples = int(segment_length * sr)
    segments = []

    for start in range(0, len(audio), segment_samples):#iterate through the audio signal in steps of segment_samples
        end = start + segment_samples
        segment = audio[start:end]
        if len(segment) == segment_samples:  # Ignore incomplete segments,Append only complete segments to the list:
            segments.append(segment)

    return segments

# Apply preprocessing to  audio files
preprocessed_audio = [(preprocess_audio(file), 16000) for file in audio_files] #Calls the preprocess_audio function for each file in audio_files.
preprocessed_audio = [x for x in preprocessed_audio if x[0] is not None]  # Remove entries with None audio

# Segment all preprocessed audio files,call the segment_audio function to generate fixed-length segments.
segmented_audio = []
for audio, sr in preprocessed_audio:
    segments = segment_audio(audio, sr)
    segmented_audio.extend(segments)

print(f"Total segments created: {len(segmented_audio)}")

"""Store the Pre-processed Audio"""

OUTPUT_PATH = "preprocessed_data"
os.makedirs(OUTPUT_PATH, exist_ok=True)

for i, segment in enumerate(segmented_audio):
    file_name = os.path.join(OUTPUT_PATH, f"segment_{i}.wav")
    sf.write(file_name, segment, sr)

print(f"Preprocessed data saved to: {OUTPUT_PATH}")

"""Visualize"""

def visualize_audio(audio, sr):
    plt.figure(figsize=(10, 4))
    librosa.display.waveshow(audio, sr=sr, alpha=0.7)
    plt.title("Audio Waveform")
    plt.xlabel("Time (s)")
    plt.ylabel("Amplitude")
    plt.show()

# Visualize a segment
visualize_audio(segmented_audio[0], sr)

"""Preprocess training and testing files"""

train_data = [preprocess_audio(file) for file in train_files]
train_data = [audio for audio in train_data if audio is not None]  # Remove None values

test_data = [preprocess_audio(file) for file in test_files]
test_data = [audio for audio in test_data if audio is not None]  # Remove None values

print(f"Number of valid training files: {len(train_data)}")
print(f"Number of valid testing files: {len(test_data)}")

# Listen to a sample of the preprocessed audio to confirm validity
import IPython.display as ipd
ipd.Audio(train_data[0], rate=16000)

"""Feature-Extraction"""

def extract_features(audio, sr=16000, n_mels=128, n_fft=2048, hop_length=512):
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)
    return librosa.power_to_db(mel_spec, ref=np.max) #transforms the Mel-spectrogram's power values to a logarithmic scale (decibels).

# Extract features , Converts the resulting list of 2D arrays (Mel-spectrograms) into a 3D numpy array
X_train = np.array([extract_features(audio) for audio in train_data])
X_test = np.array([extract_features(audio) for audio in test_data])
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)#to avoid value error ,(CNNs) expect 4D input:
# Convert labels to NumPy arrays
y_train = np.array(train_labels)
y_test = np.array(test_labels)

"""Check for data overlap"""

# Ensure no overlapping data between training and test sets
train_set = set(train_files)
test_set = set(test_files)

overlap = train_set.intersection(test_set)
if overlap:
    print(f"Overlap detected: {len(overlap)} files. Removing overlaps...")
    train_files = [f for f in train_files if f not in overlap]
    labels_train = [label for f, label in zip(train_files, labels_train) if f not in overlap]

print(f"Train set size: {len(X_train)}")
print(f"Test set size: {len(X_test)}")

"""Mel-spectogram"""

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# Function to extract Mel-spectrogram
def extract_mel_spectrogram(audio, sr, n_mels=128):
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, fmax=8000)
    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
    return log_mel_spec

# Extract and visualize Mel-spectrogram
def visualize_spectrogram(audio, sr):
    mel_spec = extract_mel_spectrogram(audio, sr)
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(mel_spec, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title("Mel-Spectrogram")
    plt.xlabel("Time (s)")
    plt.ylabel("Frequency (Hz)")
    plt.show()

# Test with a sample segment
sample_audio, sample_sr = preprocessed_audio[0]
visualize_spectrogram(sample_audio, sample_sr)

"""Build the CNN model"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.layers import BatchNormalization

from tensorflow.keras.layers import BatchNormalization

# Build the CNN with better regularization
model = Sequential([
    Conv2D(16, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.3),

    Conv2D(32, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.3),

    Flatten(),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.4),
    Dense(1, activation='sigmoid')  # Binary classification
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

y_pred = model.predict(X_test)
print(f"Predictions: {y_pred[:10]}")
print(f"Actual labels: {y_test[:10]}")

"""Train the model"""

# Train the model
# Train the model
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_labels),
    y=train_labels
)
class_weights = dict(enumerate(class_weights))

history = model.fit(
    X_train, y_train,  # Directly use training data
    batch_size=128,
    validation_data=(X_test, y_test),
    class_weight=class_weights,
    epochs=5,  # Extended epochs
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
    ]
)



# Plot training progress
plt.figure(figsize=(10, 4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training Progress')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Testing"""

test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Predict on test data
y_pred = model.predict(X_test)
y_pred_class = (y_pred > 0.8).astype("int32")  # Convert probabilities to class labels

# Visualize predictions
import random

# Pick a random test sample
index = random.randint(0, len(X_test) - 1)
test_sample = X_test[index]
pred_label = y_pred_class[index][0]
actual_label = y_test[index]

# Visualize Mel-spectrogram and prediction
plt.figure(figsize=(10, 4))
librosa.display.specshow(test_sample.squeeze(), sr=sample_sr, x_axis='time', y_axis='mel', cmap='viridis')
plt.colorbar(format='%+2.0f dB')
plt.title(f"Predicted: {'Clean' if pred_label == 1 else 'Noise'}, Actual: {'Clean' if actual_label == 1 else 'Noise'}")
plt.xlabel("Time (s)")
plt.ylabel("Frequency (Hz)")
plt.show()

import random
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Ensure y_pred contains the model's predictions (probabilities)
y_pred_prob = model.predict(X_test)
y_pred_class = (y_pred_prob > 0.5).astype("int32")  # Convert probabilities to class labels

# Randomly pick a noisy sample from the test set
index = random.randint(0, len(X_test) - 1)
test_sample = X_test[index]  # Get the spectrogram features
original_audio = test_data[index]
sample_rate = 16000  # Assuming your preprocessing uses 16kHz


# Generate the predicted and actual labels
predicted_label = "Clean" if y_pred_class[index][0] == 1 else "Noise"
actual_label = "Clean" if y_test[index] == 1 else "Noise"

# Plot the spectrogram
plt.figure(figsize=(12, 6))
librosa.display.specshow(librosa.amplitude_to_db(test_sample, ref=np.max),
                         sr=sample_rate, x_axis='time', y_axis='mel', cmap='inferno')
plt.colorbar(format='%+2.0f dB')
plt.title(f"Spectrogram\nPredicted: {predicted_label}, Actual: {actual_label}")
plt.xlabel("Time (s)")
plt.ylabel("Frequency (Hz)")
plt.show()

from sklearn.metrics import classification_report, confusion_matrix

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_class))
print("\nClassification Report:\n", classification_report(y_test, y_pred_class))

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import librosa.display

# Function to preprocess a new audio file
def preprocess_audio(file_path, sr=16000, n_mels=128, n_fft=2048, hop_length=512):
    try:
        # Load audio file
        audio, sample_rate = librosa.load(file_path, sr=sr)

        # Pad/truncate to target length
        num_samples = int(target_sr * target_length)
        if len(audio) < num_samples:
            audio = np.pad(audio, (0, num_samples - len(audio)), 'constant')
        else:
            audio = audio[:num_samples]

        # Extract Mel spectrogram features
        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to decibel scale

        # Add a channel dimension for compatibility with the model
        mel_spec_db = np.expand_dims(mel_spec_db, axis=-1)

        return mel_spec_db, audio, sr
    except Exception as e:
        print(f"Error processing audio file: {e}")
        return None, None, None

# Specify the path to your new audio file
new_audio_path = "/content/drive/MyDrive/Realtime_Recordings/samples/HVC_METRO.wav"  # Change this to your audio file path

# Preprocess the new audio file
features, raw_audio, sample_rate = preprocess_audio(new_audio_path)

if features is not None:
    # Reshape to match model input dimensions (batch size, height, width, channels)
    features = np.expand_dims(features, axis=0)

    # Predict using the model
    prediction = model.predict(features)
    predicted_label = (prediction > 0.8).astype("int32")[0][0]  # Convert to binary class (0 or 1)

    # Display the result
    print(f"Prediction: {'Clean' if predicted_label == 1 else 'Noisy'}")
    print(f"Probability: {prediction[0][0]:.2f}")

    # Visualize the Mel spectrogram of the input audio
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(features[0].squeeze(), sr=sample_rate, x_axis='time', y_axis='mel', cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title(f"Predicted: {'Clean' if predicted_label == 1 else 'Noisy'}")
    plt.xlabel("Time (s)")
    plt.ylabel("Frequency (Hz)")
    plt.show()
else:
    print("Failed to preprocess the audio file.")

"""Reducing the Background Noise"""

import numpy as np
import librosa
import matplotlib.pyplot as plt
import soundfile as sf
import scipy.fftpack as fft
from scipy.signal import medfilt

audio_file_path='/content/drive/MyDrive/Realtime_Recordings/samples/HVC_METRO.wav'
y, sr = librosa.load(audio_file_path, sr=None)
S_full,phase=librosa.magphase(librosa.stft(y))#S_full: The magnitude of the STFT (frequency content over time),The phase information (required to reconstruct the signal later).
noise_power = np.mean(S_full[:, :int(sr * 20.0)], axis=1)#Noise power is estimated by averaging the magnitude of the STFT over the initial 0.7 seconds of the audio
mask=S_full>noise_power[:,None]

mask=mask.astype(float)

mask=medfilt(mask,kernel_size=(1,5))#Applies a median filter to smooth the binary mask along the time axis

S_clean=S_full*mask
y_clean=librosa.istft(S_clean*phase)


sf.write('clean_audio.wav',y_clean,sr)


# 1. Visualize the waveforms:
plt.figure(figsize=(12, 6))
plt.subplot(2, 1, 1)
librosa.display.waveshow(y, sr=sr, label='Original')
plt.title('Original Audio')


plt.subplot(2, 1, 2)
librosa.display.waveshow(y_clean, sr=sr, label='Denoised')
plt.title('Denoised Audio')
plt.tight_layout()
plt.show()


print("Playing original audio:")
ipd.display(ipd.Audio(audio_file_path))

# 2. Play the denoised audio:
print("Playing denoised audio:")
ipd.display(ipd.Audio('clean_audio.wav'))

import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt

# Function to preprocess the new audio file
def preprocess_audio(file_path, target_sr=16000, n_mels=128, hop_length=512, n_fft=2048): # Updated target_sr
    # Load audio file
    audio, sr = librosa.load(file_path, sr=target_sr)

    # Pad/truncate to target length (make sure target_length is defined)
    target_length = 3  # Define target_length here or globally
    num_samples = int(target_sr * target_length)
    if len(audio) < num_samples:
        audio = np.pad(audio, (0, num_samples - len(audio)), 'constant')
    else:
        audio = audio[:num_samples]
    # Generate Mel-spectrogram
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length, n_fft=n_fft)
    # Convert to log scale (dB)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    # Normalize
    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / mel_spec_db.std()
    # Add a channel dimension (for compatibility with the model)
    mel_spec_db = np.expand_dims(mel_spec_db, axis=-1)
    return mel_spec_db

# Parameters (use the same as in training)
target_sr = 16000  # Sampling rate, updated to match training data
n_mels = 128  # Number of Mel bands
hop_length = 512  # Hop length
n_fft = 2048  # FFT window size

# Path to new audio file
new_audio_path = "/content/drive/MyDrive/Realtime_Recordings/samples/HVC_METRO.wav"

# Preprocess the new audio
new_audio_features = preprocess_audio(new_audio_path, target_sr, n_mels, hop_length, n_fft)

# Reshape ensuring compatibility
new_audio_features = new_audio_features.reshape(1, new_audio_features.shape[0], new_audio_features.shape[1], 1) # Updated reshape



# Predict using the trained model
y_pred_new = model.predict(new_audio_features)
predicted_class = (y_pred_new > 0.8).astype("int32")[0][0]  # Threshold to decide class

# Display the result
result = "Clean" if predicted_class == 1 else "Noise"
print(f"The audio is predicted to be: {result}")

# Visualize the Mel-spectrogram
plt.figure(figsize=(10, 4))
librosa.display.specshow(new_audio_features.squeeze(), sr=target_sr, x_axis='time', y_axis='mel', cmap='viridis')
plt.colorbar(format='%+2.0f dB')
plt.title(f"Predicted: {result}")
plt.xlabel("Time (s)")
plt.ylabel("Frequency (Hz)")
plt.show()

pip install pydub numpy scipy

from pydub import AudioSegment
from pydub.utils import make_chunks
import numpy as np
import math

def calculate_rms_and_db(audio_file_path):
    # Load the audio file
    audio = AudioSegment.from_file(audio_file_path)

    # Convert to mono for simplicity
    audio = audio.set_channels(1)

    # Extract raw audio data as an array
    samples = np.array(audio.get_array_of_samples())

    # Calculate RMS (Root Mean Square) value
    rms = math.sqrt(np.mean(samples**2))

    # Convert RMS to decibels
    db = 20 * math.log10(rms) if rms > 0 else float('-inf')  # Avoid log(0) errors

    print(f"RMS: {rms}")
    print(f"Decibels: {db} dB")
    return db

# Test the function
audio_file_path = "/content/drive/MyDrive/Realtime_Recordings/samples/HVC_METRO.wav"  # Replace with your audio file path
calculate_rms_and_db(audio_file_path)

from pydub import AudioSegment
from pydub.utils import make_chunks
import numpy as np
import math
import matplotlib.pyplot as plt

def calculate_and_plot_db(audio_file_path, chunk_size_ms=1000):
    # Load the audio file
    audio = AudioSegment.from_file(audio_file_path)

    # Convert to mono for simplicity
    audio = audio.set_channels(1)

    # Divide the audio into chunks (e.g., 1 second = 1000 ms)
    chunks = make_chunks(audio, chunk_size_ms)

    # Calculate dB levels for each chunk
    db_levels = []
    time_stamps = []
    for i, chunk in enumerate(chunks):
        # Extract samples as a NumPy array
        samples = np.array(chunk.get_array_of_samples())

        # Calculate RMS and convert to dB
        # Ensure the mean of squared samples is non-negative before taking the square root
        rms = math.sqrt(np.mean(samples**2)) if samples.size > 0 and np.mean(samples**2) >= 0 else 0
        db = 20 * math.log10(rms) if rms > 0 else float('-inf')

        # Store results
        db_levels.append(db)
        time_stamps.append(i * chunk_size_ms / 1000)  # Convert ms to seconds

    # Plot the dB levels over time
    plt.figure(figsize=(10, 6))
    plt.plot(time_stamps, db_levels, label="Decibel Level (dB)", color="blue")
    plt.xlabel("Time (seconds)")
    plt.ylabel("Decibels (dB)")
    plt.title("Noise Level Over Time")
    plt.grid(True)
    plt.legend()
    plt.show()

# Test the function
audio_file_path = "/content/drive/MyDrive/Realtime_Recordings/samples/HVC_METRO.wav"  # Replace with your audio file path
calculate_and_plot_db(audio_file_path)

from pydub import AudioSegment
from pydub.silence import detect_nonsilent
import numpy as np
import math
import matplotlib.pyplot as plt

def calculate_background_noise(audio_file_path, noise_threshold=-40, chunk_size_ms=500):
    # Load the audio file
    audio = AudioSegment.from_file(audio_file_path)

    # Normalize the audio
    audio = audio.apply_gain(-audio.max_dBFS)

    # Convert to mono
    audio = audio.set_channels(1)

    # Split audio into chunks
    chunks = [audio[i:i + chunk_size_ms] for i in range(0, len(audio), chunk_size_ms)]

    # Initialize variables
    db_levels = []
    time_stamps = []

    for i, chunk in enumerate(chunks):
        # Extract raw samples
        samples = np.array(chunk.get_array_of_samples())

        # Calculate RMS and convert to dB
        mean_squared = np.mean(samples**2)
        rms = math.sqrt(mean_squared) if mean_squared > 0 else 1e-10  # Avoid log10 errors
        db = 20 * math.log10(rms)

        # Check if the chunk contains background noise
        if db < noise_threshold:
            db_levels.append(db)
            time_stamps.append(i * chunk_size_ms / 1000)  # Convert ms to seconds

    # Plot the background noise levels
    plt.figure(figsize=(10, 6))
    plt.plot(time_stamps, db_levels, label="Background Noise Level (dB)", color="red")
    plt.xlabel("Time (seconds)")
    plt.ylabel("Decibels (dB)")
    plt.title("Background Noise Levels Over Time")
    plt.grid(True)
    plt.legend()
    plt.show()

    # Print segments with background noise
    for time, db in zip(time_stamps, db_levels):
        print(f"Time: {time}s, Decibels: {db} dB")

# Test the function
audio_file_path = "/content/drive/MyDrive/Realtime_Recordings/samples/HVC_RESTAURANT.wav"  # Replace with your audio file
calculate_background_noise(audio_file_path, noise_threshold=-40, chunk_size_ms=500)